%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Original Source: Dave Richeson (divisbyzero.com), Dickinson College
% Modified By: Chen Yiyang
% 
% A one-size-fits-all LaTeX cheat sheet. Kept to two pages, so it 
% can be printed (double-sided) on one piece of paper
% 
% Feel free to distribute this example, but please keep the referral
% to divisbyzero.com
% 
% Guidance on the use of the Overleaf logos can be found here:
% https://www.overleaf.com/for/partners/logos 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,landscape,letterpaper]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{physics}  % for vectors
\usepackage{bbm}  % for mathbb-ed digits
%\usepackage{fonts}
\usepackage{multicol,multirow}
\usepackage{spverbatim}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,urlcolor=olgreen]{hyperref}
\usepackage{booktabs}
\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{TeX Gyre Pagella}
\setsansfont{Fira Sans}
\setmonofont{Inconsolata}
\usepackage{unicode-math}
\setmathfont{TeX Gyre Pagella Math}
\usepackage{microtype}

\usepackage{empheq}

% new:
\def\MT@is@uni@comp#1\iffontchar#2\else#3\fi\relax{%
  \ifx\\#2\\\else\edef\MT@char{\iffontchar#2\fi}\fi
}
\makeatother

\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{margin=0.4in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\sffamily\large}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\sffamily\normalsize\itshape}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\itshape}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\usepackage{academicons}

\begin{document}

\definecolor{mathBlue}{cmyk}{1,.72,0,.38}
\definecolor{defOrange}{cmyk}{0, 0.5, 1, 0.3}
\definecolor{codeInlineRed}{cmyk}{0, 0.9, 0.9, 0.45}

\everymath{\color{mathBlue}}
\everydisplay{\color{mathBlue}}

% for vector notation in this module
\newcommand{\vect}[1]{\pmb{#1}}
\newcommand{\deff}[1]{\textcolor{defOrange}{\textbf{#1}}}
\newcommand{\codein}[1]{\textcolor{codeInlineRed}{\texttt{#1}}}
\newcommand{\citeqn}[1]{\underline{\textit{#1}}}

\footnotesize
%\raggedright

% \begin{center}
%   {\huge\sffamily\bfseries ST4238 Cheatsheet} \huge\bfseries\\
%   by Yiyang, AY22/23
% \end{center}
\setlength{\premulticols}{0pt}
\setlength{\postmulticols}{0pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{1.8em}
\begin{multicols}{3}


% -----------------------------------------------------------------------
\section{1. Poisson Processes}
To \textbf{homogenize a non-homogen. \textasciitilde}: [1] Define $\Lambda (t) = \int_0 ^t \lambda(u) du$ [2] $Y(s) = X(t)$ where $s = \Lambda(t)$ homogen. PoisProc. rate $1$.
\\


% \subsubsection{Law of Rare Events}
% Let $\epsilon_1, \epsilon_2, ...$ be independent Ber. r.v.'s with $P(\epsilon_i = 1) = p_i$ and let $S_n = \sum{i=1}^n \epsilon_i$. The exact probability for $S_n$ and Poisson probability with $\lambda = \sum_{i=1}^n p_i$ differ by at most
% \[
% |P(S_n = k) - e^{-\lambda} \frac{\lambda^k}{k!}| \le \sum{i=1}^n p_i^2
% \]


% TODO: ranked?
Given $X(t) = n$, the joint distribution of waiting time $W_1, ..., W_n$ is
\[
f(w_1, w_2, ..., w_n | X(t) = n) = \frac{n!}{t^n}, 0 \le w_1 \le w_2 \le ... \le 2_n \le t
\]
It is the joint distribution of $n$ \textbf{ranked} independent $Unif(0, t)$ r.v.'s.

\smallskip

Given $X(t) = n$, the distribution of the $k$-th waiting time has the same distribution as that of the $k$-th order statistic of $n$ independent $Unif(0, t)$ r.v.'s.
\[
f_k(x) = \frac{n!}{(n-k)! (k-1)!} \frac{1}{t} (\frac{x}{t})^{k-1} (1 - \frac{x}{t})^{n-k}, 0 \le k \le n
\]


Consider $\{ X(t), t \ge 0 \}$ and  $\{ Y(t), t \ge 0 \}$ two independent Poisson Processes with rates $\lambda_1$ and $\lambda_2$. Define $W_{n}^{X}$ and $W_{m}^{Y}$ as the waiting time of the $n$-th and $m$-th waiting time of $X(t)$ and $Y(t)$ respectively.
\[
P(W_{n}^{X} < W_{m}^{Y}) = \sum_{k=n}^{n+m-1} {{n+m-1}\choose{k}}    (\frac{\lambda_1}{\lambda_1 
	+ \lambda_2})^k (\frac{\lambda_2}{\lambda_1 + \lambda_2})^{n+m-1-k}
\]
\underline{Analysis}: It is equivalent as getting $n$ or more heads in $n+m-1$ tosses where getting a head has probability $\lambda_1 / (\lambda_1 + \lambda_2)$.

\smallskip

\deff{Compound Poisson Process}
\\
$E[X(t)] = \lambda t E[Y_i]$ and $Var[ X(t) ] = \lambda t \Big( E[Y_i]^2 +  Var(Y_i) \Big)$
\[
\lambda = \lambda_1 + \lambda_2
\, 
F(x) = \frac{\lambda_1}{\lambda_1 + \lambda_2} F_1(x) + \frac{\lambda_2}{\lambda_1 + \lambda_2} F_2(x)
\]


\deff{Conditional Poisson Process}
\\
$E[N(t)] = E(L) t$ and $Var(N(t)) = t E(L) + t^2 Var(L)$
\\
Conditional probability of $L$ given $N(t) = n$ (posterior),
\[
P(L \le x | N(t) = n) = \frac{
	\int_{0}^x e^{-\lambda t} (\lambda t)^n g(\lambda) d \lambda
}
{
	\int_{0}^\infty e^{-\lambda t} (\lambda t)^n g(\lambda) d \lambda}
\]



% ------------
\section{2. Continusous Time Markov Chains}
\subsection{Overview}
\underline{Note}: For an absorbing state $i$, we may set $\nu_i = 0$.
\\
Define the \deff{Transition Probabilities} of a CTMC $X(t)$ as
\[
P_{ij}(t) := P(X(t+s) = j | X(s) = i)
\]
\underline{Note}: [1] $P(t)$ uniquely specifies a CTMC. [2] $P_{ij} \neq P_{ij}(t)$.


\subsubsection{Chapman-Kolmogorov Equation}
\[
P_{ij} (t + s) = \sum_{k \in S} P_{ik} (t) P_{kj} (s)
\]

% \subsubsection{Poisson Process as CTMC}
% A Poisson Process $\{ X(t), t \ge 0 \}$ with rate $\lambda$ can be modelled as a CTMC with state space $S = \{ 0, 1, 2, ... \}$, rates $\nu_i = \lambda, \forall i \in S$, and jump matrix $P$ where $P_{i,i+1} = 1$ and $P_{ij} = 0, \forall j \neq i + 1$.
% \\
% \underline{Note}: CTMCs are not necessarily Poisson Processes.


\subsubsection{Discretisation of CTMC}

For a CTMC $\{ X(t), t \ge 0 \}$, $\{Y_1(n) \}_{n \ge 0}$ discretises it at equal intervals if for some constant $l > 0$,
\[
Y_1(n) = X(nl), n = 0, 1, 2, ...
\]
\underline{Analysis}: $Y_1(n)$ has state space $S$ and transition matrix $P(l)$.

\smallskip

For a CTMC $\{ X(t), t \ge 0 \}$, $\{Y_2(n) \}_{n \ge 0}$ is the \deff{Embedded Chain} if it only considers the states visited by $X(t)$.
\\
\underline{Analysis}: $Y_2(n)$ has state space $S$ and transition matrix $P$.


\subsection{Infinitesimal Generator}
Lemma: Transition Rates, for a CTMC,
\begin{itemize}
	\item $\lim_{h \to 0} \frac{P_{ii}(h) - P_{ii}(0)}{h} = - \nu_i$
    \item $\lim_{h \to 0} \frac{P_{ij}(h) - P_{ij}(0)}{h} = \nu_i P_{ij}$, for all $i \neq j$
\end{itemize}

% For states $i \neq j \in S$, define \deff{Instantaneous Transition Rates} as
% \[
% q_{ij} := \nu_i P_{ij}
% \]

The \deff{Infinitesimal Generator} $G$ of a CTMC is defined as
% \[
% G_{ii} = - \nu_i, \; G_{ij} = q_{ij}, \; i \neq j 
% \ \text{ and }
% P'(0) = G
% \]


\[
G = (G_{ij})_{S \times S} = P'(0),
\text{ where }
G_{ij} = \begin{cases}
    -\nu_i, &i=j
    \\
    \nu_i P_{ij}, &i\neq j
\end{cases}
\]


\deff{Kolmogorov's Forward \& Backward Equations} respectively:
\[
\begin{aligned}
P'(t) = P(t)G
&\iff
P_{ij}'(t) = \sum_{k \neq i} P_{ik}(t)q_{kj} - \nu_j P_{ij}(t)
\\
P'(t) = GP(t)
&\iff
P_{ij}'(t) = \sum_{k \neq i} q_{ik}P_{kj}(t) - \nu_i P_{ij}(t)
\end{aligned}
\]

% \subsubsection{Kolmogorov's Backward Equations}
% For all states $i, j$, and times $t \ge 0$,
% \[
% P'(t) = GP(t)
% \iff
% P_{ij}'(t) = \sum_{k \neq i} q_{ik}P_{kj}(t) - \nu_i P_{ij}(t)
% \]
% \underline{Note}: $G$ uniquely decides $P(t)$.



\subsection{CTMC Long-Term Properties}
\subsubsection{Stationary Distribution}
For a CTMC $\{X(t), t \ge 0\}$, a row vector $\pmb{\pi} = (\pi_i)_{i \in S}$ with $\pi_i \ge 0$ and $\sum_i \pi_i = 1$ is a \deff{Stationary Distribution} if for all $t \ge 0$,
\[
\pmb{\pi} = \pmb{\pi} P(t), 
\]
\deff{Global Balancing Equations}
\[
\pmb{\pi} G = \pmb{0}
\iff
\equiv \sum_{j\neq i} \pi_i q_{ij} = v_j \pi_j, \ \forall j
\]


\subsubsection{Limiting Distribution}
For a CTMC $\{X(t), t \ge 0\}$, its \deff{Limiting Distribution}, $\{ P_j, j \in S \}$, is:
\[
P_j = \lim_{t \to \infty} P_{ij}(t)
\]

\underline{Note}: [1] For each $j$, the limit needs to exist and be the same for all $i$. [2] When both $\pmb{\pi}$ and $P$ exists, $\pmb{\pi} = P$.

\smallskip

If $X(t)$ satisfies conditions below, it is \deff{Ergodic} (converse not true): [1] All states of $X(t)$ \textbf{communicate}. [2] $X(t)$ is \textbf{positive recurrent}, i.e. for all $i, j \in S$, $\mathbb{E}[\min_{t \ge 0} \{ X(t) = j | X(0) = i \}] < \infty$ 
\\
An ergodic chain has stationary \& limiting distributions \& equal.

\smallskip

Suppose embedded chain stationary distribution $\pmb{\psi}$. Then $\forall i, j \in S$,
\[
\psi_i = \frac{\pi_i \nu_i}{\sum_{j} \pi_j \nu_j}
\iff
\pi_i = \frac{\psi_i / \nu_i}{\sum_{j} \psi_j / \nu_j}
\]


\subsubsection{Time Reversibility}
For an \textbf{ergodic} CTMC $\{X(t), t \ge 0\}$ and a sufficiently large $t$, define reversed process $\{Y(t), t \ge 0\}$
\[
Y(0) = X(t), \ Y(s) = X(t-s), 0 < s < t
\]
$\{X(t), t \ge 0\}$ is \deff{Time-Reversible} if $X(t)$ and $Y(t)$ has the same probability structure: [1] Same $\pmb{\nu}$, and [2] Same jump matrix.

\smallskip

\deff{Local Balanced Equations}
\[
\pi_j q_{ji} = \pi_i q_{ij}, \ \forall i, j
\]
If it is satisfied, $X(t)$ is time reversible with limiting distribution $\pmb{\pi}$.

\smallskip

\textbf{Proposition: Time Reversibility Subset}
\\
Truncate a time-reversible CTMC $X(t)$ from $S$ to $A \subseteq S$, then it remains time-reversible and has limiting distribution
\[
\pi_j^A = \frac{\pi_j}{\sum_{i \in A} \pi_i}, \ \forall j \in A
\]

\textbf{Proposition: Time Reversibility Vectors}
\\
For CTMCs $\{ X_i(t), t \ge 0 \}, i = 1, 2, ..., n$ time reversible, the vector process $\{(X_1(t), ..., X_n(t)), t \ge 0 \}$ is also time reversible.




\subsection{CTMC Techniques}
\subsubsection{Uniformization}
For a CTMC $\{X(t), t \ge 0\}$, where $\exists \nu \in \mathbb{R}$ s.t. $\nu_i \le \nu, \forall i \in S$,
\[
P_{ij}(t) = \sum_{n=0}^\infty (P^*)_{ij} \frac{(\nu t)^n}{n!} e^{-\nu t}
\ 
\text{ ,where }
P_{ij}^* = 
\begin{cases}
1 - \nu_i / \nu, & i = j
\\
(\nu_i / \nu) P_{ij}, & i \neq j
\end{cases}
\]
\underline{Intuition}: $P^*$ is the jump matrix after \deff{Uniformisation}. A CTMC with identical $\nu_i$ is a Poisson Process with rate $\nu_i$.



\subsubsection{CTMC with Absorbing States}
For a CTMC $\{X(t), t \ge 0\}$, if there is a state $i$ s.t. $\forall t > 0, s \ge 0$,
\[
P(X(t+s) = i | X(s) = i) = 1
\]
, (or $P_{ii}(t) = 1, \forall t>0$,) then we call $i$ an \deff{Absorbing State}.

\smallskip

Assume state $0$ is absorbing, \textbf{probability of absorbing} $u_i = \lim_{t \to \infty} P(X(t) = 0 | X(0) = i)$ from state $i$ by CTMC is \textbf{the same as that based on its embedded chain}.
\\
Define \textbf{expected time of absorption} $w_i$ for starting at state $i$. \underline{Case 1} When $i = 0$, $w_i = 0$. \underline{Case 2a} When $u_i < 1$,  $w_i = \infty$. \underline{Case 2b} When $u_i = 1$, $w_i = \mathbb{E}[\text{time till 1st jump}] + \sum_{j \neq i} P_{ij}w_j$.





\section{3. Renewal Process}
\subsection{Overview}
A \deff{Renewal Process} is a counting process $\{ N(t), t \ge 0 \}$ for a sequence of non-negative r.v.s $\{X_1, X_2, ... \}$ that are iid. with a distribution $F$.
\begin{itemize}
    \item $F(x) = P(X_k \le x), k = 1, 2, ...$, CDF of sojourn time
    \item $F_k(x) = P(W_k \le x), k=1, 2, ...$, CDF of waiting time $W_k$.
    \item $M(t) = E[N(t)]$, \deff{Renewal Function}, expected \# of renewals
\end{itemize}

Properties
\begin{itemize}
    \item $N(t) \ge k \iff W_k \le t$
    \item $W_{N(t)} \le t < W_{N(t)+1}$
    \item $P(N(t)=k) = F_k(t) - F_{k+1}(t)$
    \item $F_k(t) = \int_{0}^t F_{k-1}(t-y)dF(y)$, one-step analysis
\end{itemize}


\deff{The Renewal Equation} - For a renewal process with sojourn times distributed as $F$, let $M(t) = E[X(t)]$, then
\[
M(t) = F(t) + \int_{0}^t M(t-x)f(x)dx
\]

\smallskip

Waiting time in Renewal Process:
\[
\mathbb{E}[W_{N(t)+1}] = \mathbb{E}[X_1](M(t)+1)
\]
\underline{Note}: Not a Random Sum as $N(t)+1$ not independent with $X_i$.


\subsubsection{Special Random Variables}
\begin{itemize}
    \item \deff{Excess Time / Residual Time}: $\gamma_t = W_{N(t)+1} - t$
    \item \deff{Current Life / Age}: $\delta_t = t - W_{N(t)} \ge 0$
    \item \deff{Total Life}: $\beta_t = \gamma_t + \delta_t$
\end{itemize}

Special Case: Poission Distribution
\begin{itemize}
    \item $\gamma_t \sim Exp(\lambda)$
    \item $\delta_t$ follows $Exp(\lambda)$ truncated at $t$.
    \item $\mathbb{E}[\beta_t] = 1/\lambda + (1-\exp(-\lambda t)) / \lambda$
\end{itemize}


\subsubsection{Limiting Behaviours}
\deff{Elementary Renewal Theorem}
\[
\lim_{t \to \infty} \frac{N(t)}{t} = \frac{1}{\mathbb{E}[X_k]} 
\]

\deff{Central Limit Theorem} for Renewal Process
\\
Let $\mu = E(X_k), \sigma^2 = \text{Var}(X_k)$, then as $t \to \infty, \frac{\text{Var}(N(t))}{t} \to \frac{\sigma^2}{\mu^3}$ and 
\[
N(t) \sim \mathcal{N}(\frac{t}{\mu}, \frac{t \sigma^2}{\mu^3}) \text{ approximately} 
\]




\subsection{Generalisation}
\subsubsection{Renewal Reward Process}
Given a renewal process $N(t)$ with interarrival times $X_n, n \ge 1$ and suppose there is a reward for each renewal $R_n$ that are i.i.d., the \deff{Renewal Reward Process} $\{ R(t), t \ge 0 \}$ is
\[
R(t) = \sum_{n=1}^{N(t)} R_n
\]
\underline{Note}: [1] $R_n$ can depend on $X_n$, time. [2] Rewards can occur between/along renewals.

\smallskip

\textbf{Limiting Theorems} for Renewal Reward Process
\\
For $E[R_n] < \infty$ and $E[X_n] < \infty$,
\[
\lim_{t\to \infty} \frac{E[R(t)]}{t} = \frac{E[R_n]}{E[X_n]}
\]

\underline{Example}: Avg. Current Life $\lim_{t\to\infty} (\int_{0}^t \delta(s)ds) / t = E[X^2] / 2E[X]$.



\subsubsection{Regenerative Process}
A stochastic process $\{X(t), t\ge 0 \}$ is a \deff{Regenerative Process} if there exists time pts when the process probabilistically restarts itself.
\\
\underline{Note}: [1] "Restart" includes both same transition \& whether current state is same as initial. [2] Neither of MC \& RegenProc $\subseteq$ the other.


\subsubsection{Delayed Renewal Process}
A \deff{Delayed Renewal Process} is one when the component in operation at $t = 0$ is not new, but all subsequent ones are.
\\
\underline{Analysis}: Same set of parameters \& limiting behaviours.



\section{4. Brownian Motion}
\subsection{Multi-Normal Distribution}
A $k$-dim random vector $\mathbf{X}=(X_1, ..., X_k)'$ with mean vector $\mu \in \mathbb{R}_{k\times 1}$ and covariance matrix $\Sigma \in \mathbb{R}_{k \times k}$ is \deff{multivariate normally distributed} $\mathbf{X} \sim \mathcal{N}(\mu, \Sigma)$ if the joint density function is
\[
f(x_1, ..., x_k) = \frac{1}{\sqrt{2\pi \lVert \Sigma \rVert}} \exp\Big( 
-\frac{1}{2} (\mathbf{x}-\mu)' \Sigma^{-1} (\mathbf{x}-\mu)
\Big)
\]

\smallskip

For any $\mathbf{a} \in \mathbf{R}_{1\times k}$, $aX \sim \mathcal{N}(a\mu, a\Sigma a')$.
\\
For any matrix $\mathbf{Q} \in \mathbf{R}_{m \times k}$ with rank $m \le k$, $QX \sim \mathcal{N}(Q\mu, Q\Sigma Q')$.
\\
For any parition,
\[
\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{pmatrix} \sim \mathcal{N}( 
\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, 
\begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix})
\]
And the conditional distribution is still normal:
\[
\mathbf{X}_1 | \mathbf{X}_2 \sim \mathcal{N}(
\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}(\mathbf{X}_2-\mu_2), \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
)
\]



\subsection{Overview}
\subsubsection{(Standard) Brown Motion}
Process $\{X(t), t \ge 0 \}$ is a \deff{Brownian Motion} with parameter $\sigma$ if: [1] $X(0) = 0$, [2] $\{ X(t), t \ge 0 \}$ has stationary \& independent increments, and [3] For every $t > 0$, $X(t) \sim \mathcal{N}(0, \sigma^2 t)$.
\\
A \deff{Standard Brownian Motion} $\{ B(t), t \ge 0 \}$ has $\sigma = 0$.

\smallskip

For time $t_1 \le t_2$,
\[
\begin{pmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{pmatrix}
\sim \mathcal{N}( \mathbf{0}, 
, 
\begin{pmatrix} \sigma^2 t_{1}  & \sigma^2 t_{1} \\ \sigma^2 t_{1} & \sigma^2 t_{2} \end{pmatrix})
\]

For any time $s, t > 0$,
\begin{itemize}
    \item If $s \ge t$, $X(s)|X(t) \sim \mathcal{N}(X(t), \sigma^2(s-t))$.
    \item If $s<t$, $X(s)|X(t) \sim \mathcal{N}(
    \frac{s}{t}X(t), \sigma^2s^2 - \sigma^2s^2/t^2)$ 
\end{itemize}


\subsubsection{Brownian Motion with Drift}
A \deff{Brownian Motion with Drift} $\{ X(t), t \ge 0 \}$ with parameters $\mu$ and $\sigma$: $X(t) = \sigma B(t) + \mu t$.

\smallskip

For any time $s, t > 0$,
\[
X(s) | X(t) \sim \mathcal{N}(
\mu, \frac{\min(s, t)}{t} [X(t) - \mu t], \sigma^2 s - \sigma^2 [\min(s, t)]^2 / t)
\]


\subsubsection{Geometric Brownian Motion}
A \deff{Geometric Brownian Motion} $Y(t)$ with parameters $\mu$ and $\sigma$ is:
\[
Y(t) = e^{\sigma B(t) + \mu t}
\]
\underline{Note}: $Y(0) = 1$ and $Y(t) \ge 0, \forall t$.

\smallskip
For any time $s < t$,
\[
\begin{aligned}
\mathbb{E}[Y(t)] &= M_{X(t)}(1) = e^{\mu t + \sigma^2 t/2}
\\
\text{Var}(Y(t)) &= M_{X(t)}(2) - (M_{X(t)}(1))^2 = e^{2\mu t + \sigma^2 t} (e^{\sigma^2 t} - 1)
\\
\mathbb{E}[Y(t) | Y(s)] &= Y(s)\exp(\mu (t-s) + \sigma^2 (t-s)/2)
\\
\text{Cov}(Y(s), Y(t)) &= \exp(\mu (t+s) + \sigma^2(t+s)/2) (\exp(\sigma^2 s)-1)
\end{aligned}
\]





\noindent\rule{8cm}{0.4pt}

\section{Intermediate Results \& Others}
\subsection{From Lecture \& Tutorials}
For \deff{Birh \& Death Process} with +1 $\lambda_i$ \& -1 $\mu_i$, limiting distribution:
\[
\pi_n = \pi_0 \prod_{i=1}^{n} \frac{\lambda_{i-1}}{\mu_i}, \ \text{ subject to }
\sum_{n=0}^{\infty} \pi_n = 1
\]

\smallskip

\textbf{Delay Renewal Example}: Consider $Y_1, Y_2, ...$ iid. A \deff{Pattern} is a $r$-dim vector $(y_1, ..., y_r)$. Every time $(Y_{n-r+1}, ..., Y_n) = (y_1, ..., y_r)$, a renewal occurs at $n$, denoted as $I(n) = 1$.  The counting process $N(n)$ is a Delayed Renewal Process.
\\
Define \deff{Overlapping} $k = \max \{ j< r: (y_{r-j+1}, ..., y_r) = (y_1, ..., y_j) \}$ how much two renewals overlap. Let $p = P(I(n)=1)$. 
\\
When $k = 0$: $E[X_1] = 1/p$,  $\text{Var}(X_1) = 1/p^2 - (2r-1)/p$.
\\
When $k > 0$: $E[X_1] = E[X_{y_1, ..., y_k}] + 1/p$, and $\text{Var}(X_1) = \text{Var}(X_{y_1, ..., y_k}) + p^{-2} - (2r-1)/p + 2p^{-3} \sum_{j=r-k}^{r-1} E[I(r)I(r+j)]$.

\smallskip

\citeqn{(Tut5Qn2)} For $X_1 \sim Exp(\lambda_1)$ and $X_2 \sim Exp(\lambda_2)$ independent, we have $\min(X_1, X_2) \sim Exp(\lambda_1 + \lambda_2)$.



\noindent\rule{8cm}{0.4pt}

\subsection{Appendix: Probability Theory}
% \subsection{Multi-Var Normal Distribution}

\subsubsection{Gamma Distribution}
$X \sim Gamma(\alpha, \lambda)$ for shape $\alpha$, and rate $\lambda > 0$. ($1/\lambda$ scale param.)
\[
f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\lambda x}, \quad x \geq 0
\]
Statistics: $E(X) = \frac{\alpha}{\lambda}, \ \text{var}(X) = \frac{\alpha}{\lambda^2}$ \\
MGF: $M_X(t) = \left(1 - \frac{t}{\lambda}\right)^{-\alpha},\quad t < \beta$ \\
Special case: $Exp(\lambda) = Gamma(1, \lambda)$, $\chi^2_n = Gamma(\frac{n}{2}, \frac{1}{2})$ \\
Properties: $Gamma(a, \lambda) + Gamma(b, \lambda) = Gamma(a+b, \lambda)$, and $cX \sim Gamma(\alpha, \frac{\lambda}{c})$
\\
\textbf{Gamma function} $\Gamma(\alpha) = \int_0^{\infty} e^{-y}y^{\alpha-1} \, dy$ \\
$\Gamma(1) = 1, \ \Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}$ and $\Gamma(n) = (n-1)!, \ n \in \mathbb{Z}^{+}$


\subsubsection{Beta Distribution}
$X \sim B(a, b)$ where $a>0, b>0$ has support $[0, 1]$
\[
f(X) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1} (1-x)^{b-1}, 0 \le x \le 1
\]
Statistics: $E(X) = \frac{1}{1 + \beta / \alpha}, \ \text{var}(X) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$
\\
Special case: $\text{Unif}(0, 1) = B(1, 1)$

\textbf{Beta function} $B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} \ dt = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$

\subsubsection{Common MGFs}
Binomial: $M_X(t) = (1-p+pe^t)^n$
\\
Poisson: $M_X(t) = \exp(\lambda (e^t - 1))$
\\
Exponential: $M_X(t) = \frac{\lambda}{\lambda - t}$ for $t < \lambda$
\\
Normal: $M_X(t) = \exp(\mu t + \sigma^2 t^2 / 2)$

\subsubsection{Others}
$E[(X-\mu)^4] = 3\sigma^4$ for $X \sim \mathcal{N}(\mu, \sigma^2)$
\\
$E[(\int_{0}^{T} B(s)ds)^2] = \int_{0}^{T} \int_{0}^{T} E[B(s)B(t)] dtds $

% TODO: normal (for geom brownian)!

% \section{Intermediate Results \& Lemmas}
% \subsection{From Tutorials}










% -----
% KIV:
% 1. conversion between homogeneous.


\end{multicols}
\end{document}
