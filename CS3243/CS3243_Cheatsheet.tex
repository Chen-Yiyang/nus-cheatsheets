%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Original Source: Dave Richeson (divisbyzero.com), Dickinson College
% Modified By: Chen Yiyang
% 
% A one-size-fits-all LaTeX cheat sheet. Kept to two pages, so it 
% can be printed (double-sided) on one piece of paper
% 
% Feel free to distribute this example, but please keep the referral
% to divisbyzero.com
% 
% Guidance on the use of the Overleaf logos can be found here:
% https://www.overleaf.com/for/partners/logos 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,landscape,letterpaper]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{physics} % for vectors
%\usepackage{fonts}
\usepackage{multicol,multirow}
\usepackage{spverbatim}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{listings} % for code block
\usepackage[colorlinks=true,urlcolor=olgreen]{hyperref}
\usepackage{booktabs}
\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{TeX Gyre Pagella}
\setsansfont{Fira Sans}
\setmonofont{Inconsolata}
\usepackage{unicode-math}
\setmathfont{TeX Gyre Pagella Math}
\usepackage{microtype}

\usepackage{empheq}

% new:
\def\MT@is@uni@comp#1\iffontchar#2\else#3\fi\relax{%
  \ifx\\#2\\\else\edef\MT@char{\iffontchar#2\fi}\fi
}
\makeatother

\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{margin=0.4in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\sffamily\large}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\sffamily\normalsize\itshape}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\itshape}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\usepackage{academicons}

\begin{document}

\definecolor{mathBlue}{cmyk}{1,.72,0,.38}
\definecolor{defOrange}{cmyk}{0, 0.5, 1, 0.3}
\definecolor{codeInlineRed}{cmyk}{0, 0.9, 0.9, 0.45}
\everymath{\color{mathBlue}}
\everydisplay{\color{mathBlue}}


% Based on:
% https://stackoverflow.com/questions/3175105/inserting-code-in-this-latex-document-with-indentation
\definecolor{codedkgreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codemauve}{rgb}{0.58,0,0.82}
\definecolor{light-gray}{gray}{0.95}

\lstset{frame=tb,
  language=Java,
  backgroundcolor=\color{light-gray},
  basicstyle={\footnotesize\ttfamily},
  tabsize=3
}


% for vector notation in this module
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\deff}[1]{\textcolor{defOrange}{\textbf{#1}}}
\newcommand{\codein}[1]{\textcolor{codeInlineRed}{\texttt{#1}}}
\newcommand{\citeqn}[1]{\underline{\textit{#1}}}

\footnotesize
%\raggedright

\graphicspath{ {./img/} }


\begin{center}
  {\huge\sffamily\bfseries CS3243 Cheatsheet} \huge\bfseries\\
  by Yiyang, AY21/22
\end{center}
\setlength{\premulticols}{0pt}
\setlength{\postmulticols}{0pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{1.8em}
\begin{multicols}{3}




% -----------------------------------------------------------------------

\section{Introduction}

\subsection{Intelligent Agent}
An intelligent agent consists of: (1) \deff{Sensors}, for capturing data (known as \deff{Percepts}, $p_i$ of \deff{Percept History}, $P=\{p_1, ..., p_n, ... \}$) from the environment; (2) \deff{Agent Functions}, $f$, for making decisions based on percepts, and \textbf{Actuators}, for performing actions, $a_1, ..., a_m \in A$, based on agent functions.

In short, an agent is a function $f: P \to A$.

\subsection{Types of Agents and Environments}
\subsubsection{Agents}
\deff{Reflex Agent} - Agents that use IF-ELSE rules to make decisions. \\
\deff{Model-based Reflex Agent} - Agents that use an internalised model to make decisions. (e.g. state graph models for search AI) \\
\deff{Goal-/Utility-based Agent} - Agents that determine sequences of actions to reach goals / maximise utility. \\
\deff{Learning Agent} - Agents that learn to optimise. (not covered)


\subsubsection{Properties of the Problem Environments}
\deff{Fully} / \deff{Partially Observable} - whether agents can access all information of the environment. \\
\deff{Deterministic} / \deff{Stochastic} \textasciitilde \ whether transition of states is certain. \\
\deff{Episodic} / \deff{Sequential} - whether actions impact only current states or all future decisions. \\
\deff{Discrete} / \deff{Continuous} \textasciitilde \ of state info., time, percepts and/or actions. \\
\deff{Single} / \deff{Multi-agent} \\
\deff{Known} / \deff{Unknown} \textasciitilde \ of knowledge of the agent. \\
\deff{Static} / \deff{Dynamic} - whether the environment changes while the agent is deciding actions.

\smallskip

The real world is partially observable, stochastic, sequential, dynamic, continuous, multi-agent.


\section{Uninformed Search}
\subsection{General Search}
\subsubsection{Search Problem Definitions}
A Search problem consists of 1) \deff{State representation}, $s_i$, for each environment instance, 2) \deff{Goal test}, $\text{isGoal } : s_{i} \to \{ 0, 1\}$, that determines if a state is a goal, 3) \deff{Action Function}, $\text{action } : s_{i} \to A$, that returns possible actions for every state, 4) \deff{Action Cost}, $\text{cost } : (s_{i}, a_{j}, s^{'}_{i}) \to V$, that returns cost $v$ of taking action $a_{i}$ of state $s_{i}$ to reach $s^{'}_{i}$, and 5) \deff{Transition Model}, $T : (s_{i}, a_{j}) \to s^{'}_{i}$ representing the state transition.
\smallskip

A transition model describes the problem in a dynamic and efficient way, as it does not list all states' actions.
\smallskip

\deff{Uninformed Search} are search algorithms without domain knowledge beyond the search problem formulation.


\subsubsection{Generic Search Algorithm}
The only difference is how each algorithm implements the \codein{frontier} for searching. 

\smallskip

\textbf{Correctness:}: 1) \deff{Completeness} - whether an algorithm will find a solution when one exists \textbf{AND} correctly report failure if not exists, and 2) \deff{Optimality} - whether an algorithm finds a solution with lowest path cost among all solutions.
Note: An optimal solution must be complete.
\smallskip

Implementation wise, there are 1) \deff{Tree-Search} that allows revisiting of nodes, and 2) \deff{Graph-Search} that do not allow revisit to states unless the new cost is smaller than current one (by maintaining a \codein{reached} hash table upon adding nodes to \codein{frontier}. 

\subsection{Search Algorithms}
\deff{Breadth-First Search} (BFS) : Use \codein{Queue<>} for \codein{frontier}. Possible improvement is to perform \textbf{Goal checking on pushing to frontier} to reduce storage.
\\
\deff{Depth-First Search} (DFS) : Use \codein{Stack<>} for \codein{frontier}.
\\
\deff{Uniform-Cost Search} (UCS) : Use \codein{PriorityQueue<>} for \codein{frontier}. Essentially Dijkstra's Algorithm that always explores the node with shortest path cost in the frontier. \underline{Note}: Need to ensure all costs are larger than some constant $\epsilon > 0$. Therefore, it cannot be used for negative / zero costs (just like Dijkstra).
\\
\deff{Depth-Limited Search} (DLS) : DFS but with a limit on the maximum depth, $l$, which may be determined using domain knowledge.
\\
\deff{Iterative Deepening Search} (IDS) : Perform DLS repeated with $l = 1, 2, ...$. Intuitive, the algorithm compromises running time for better memory usage.




\section{Informed Search}
\subsection{Heuristics}
\deff{Heuristic Function}, $h = h(n)$, approximates the shortest distance from a state to the nearest goal.\\
$h(n)$ tries to approximate the actual distance function $h^*(n)$.
\\
A heuristic is \deff{admissible} if for any state $n$, $ h(n) \leq h^{*}(n) $ , which means the heuristic might under-estimate but never over-estimates.
\\
A heuristic is \deff{consistent} if for all states $n$ and its successor $n'$, $h(n) \leq \text{cost}(n, a, n') + h(n')$ , which means priority $f$ in A* is non-decreasing along a path if a consistent heuristic is chosen. \underline{Note}:Consistent heuristics are always admissible.
\\
Two heuristics, $h_1$ \deff{dominates} $h_2$ iff. $h_1(n) \geq h_2(n)$ for all states $n$. \\
Note: dominance is defined for all heuristics. However, for two admissible heuristics, the dominating one is preferred.




\subsection{Informed Search Algorithms}
Based on UCS, but incorporate domain knowledge via $h$.

\smallskip

\deff{Greedy Best-First Search} : Use \deff{Evaluation Function}, $f(n) = h(n)$ as priority. Intuitively, it picks state "seemingly" closest to goal.
\\
\underline{Analysis}: \textbf{Incomplete} under Tree-Implementation, and \textbf{Complete} under Graph-Implementation. \textbf{Not optimal} under both implementations.

\smallskip

\deff{A* Search} : Use \deff{Evaluation Function}, $f(n) = g(n) + h(n)$ as priority where $g(n)$ is the current path cost.
\\
\deff{Limited-Graph Search} (LGS) : A modified Graph-Implementation of A* that adds nodes to \codein{reached} table on pop instead of pushing.
\\
\underline{Analysis}: Tree-Implementation of A* is \textbf{Optimal} for admissible $h$. LGS is \textbf{Optimal} for consistent $h$.




\section{Local Search}
\deff{Local Search} only concerns with goal state(s) but not how it is found or its cost. 
\\
Local Search is \textbf{Incomplete}, but it uses less space ($O(b)$ for branching factor $b$) and is applicable to larger and finite search space.
\\
\deff{Complete-State Formulation} - Every state has all components of a solution. Each state is a potential solution.

\subsection{Local Search Algorithm}
\deff{Hill Climbing} (aka. \deff{Steepest Ascent - Greedy Strategy}) - It stores only current state. In each iteration, find a successor that improves - 1) use actions and transitions to determine successors, and 2) use "heuristic-liked" values (e.g. $f(n) = -h(n)$) to evaluate each state. The algorithm terminates with a state when the value $f$ cannot be improved. \underline{Note}: The algorithm may fail as it can terminate at a local maximum / plateau.

\subsubsection{Hill Climbing Variations}
\deff{Slideways Move} - Replace $<$ with $\leq$, to allow continuation with neighbours of same value and to traverse plateaus. \\
\deff{Stochastic \textasciitilde} - Choose randomly a state with better value (not the best value) to explore. This takes longer time to find a solution but gives more flexibility and randomness. Relieve "local maximum" issue. \\
\deff{First-Choice \textasciitilde} - Generate successors until one with better value than current is found. \\
\deff{Random-Restart \textasciitilde} - Use a loop to randomly pick a new starting state. Keep running until a solution is found.


\subsubsection{Local Beam Search}
\deff{Local Beam Search} - Similar to Hill Climbing but start with $k$ random starting states. Each iteration generates successors of all $k$ states and choose new $k$ ones to explore. Stochastic elements can also be incorporated into this algorithm. \underline{Note}: {[1]} It is not equivalent to $k$-parallel Hill Climbing. {[2]} Local Beam requires problems with different possible starting states, otherwise it cannot be run.



\section{Constraint Satisfication Problem (CSP)}
\subsection{CSP Formulation}
A CSP consists of 1) \deff{State Representation}, for variables $X = \{ x_1, ..., x_n \}$ and set of domains $D = \{ d_1, ..., d_n \}$, so that $x_i$ has domain $d_i$, 2) \deff{State}, where the initial / intermediate / goal state(s) have variables unassigned / partially assigned / all assigned, 3) \deff{Goal Test}, with constrains $C = \{c_1, ..., c_m \}$ to satisfy, and 4) \deff{Actions} and \deff{Transitions}. \underline{Note}: Costs are not utilised in CSP.

\smallskip

Depending on the \deff{Scope}, number of variables involved, of constraints, they can be categorised into 1) \deff{Unary}, \deff{Binary}, and \deff{Global} which involves $1$, $2$, and $\geq 3$ variables respectively.
\smallskip

A \deff{Constraint Graph} is a representation for constraints where each variable is a vertex, each binary constraint is an edge between two variables, and each global constraint is expressed as multiple binary constraints using linking a vertex.

\subsection{CSP Heuristics}
\deff{Minimum-Remaining-Values Heuristic} (MRV) - Choose the variable with fewest legal values to be assigned. \underline{Intuitively}, it means to consider nodes with fewer branches first, so that it tends to prone larger invalid sub-trees.
\\
\deff{Degree Heuristic} - Choose the variable with fewest number of constraints with other non-determined variables (smallest \deff{degree}) to be assigned. Typically used as a tie-breaker after MRV Heuristic.
\\
\deff{Least-Constraining-Value Heuristic} (LCV) - Choose value that rules out fewest choices (in terms of domain sizes of "neighbour variables") to be assigned. \underline{Intuitively}, it avoids failure.

\smallskip

\deff{Variable Order Heuristics}: MRV \& Degree. \deff{Value Order Heuristics}: LCV

\subsection{CSP Algorithms}
\deff{Backtracing Algorithm} uses DFS to examine value assignments of one variable at a time, and backtrack when no possible legal assignments with current assignment of the variable.
\\
Heuristics can be used for \codein{SELECT-UNASSIGNED-VARIABLE()} and \codein{ORDER-DOMAIN-VALUES}. Constraint propagation algorithms are used for \codein{INFERENCE()}.

\subsubsection{Constraint Propagation Algorithms}
\deff{Forward Checking} tracks remaining legal values (reduces domain) of each unassigned variable after current variable's assignment. It makes search terminate when any variable has no legal values (in its domain).

\smallskip

\deff{AC3 Algorithm} updates all arcs corresponding to binary constrains of that variable with others, whenever the domain of the variable is updated. It ensures \deff{Arc-Consistency} between variables $X_i$ and $X_j$ that is for every $x \in D_i$ there exists some value $y \in D_j$ that satisfies the binary constraint(s) on arc $(X_i, X_j)$. \underline{Note}: Arcs are \textbf{directed}.
\\
\underline{Analysis}: Time $O(n^2 d^3)$ for number of states $n$ and domain size $d$. Preprocessing to reduce domain size is therefore important.




\section{Adversarial Search}
\subsection{Game Model}
A game model consists of 1) \deff{State representation}, 2) \codein{TO-MOVE(s)}, a function that returns the player to move for state $s$, 3) \codein{ACTIONS}, legal moves in state $s$, 4) \codein{RESULT(s, a)}, the transition model that returns resultant state after taking action $a$ at state $s$, 5) \codein{IS-TERMINAL(s)}, whether the game has ended at state $s$, and 6) \codein{UTILITY(s, p } that defines the final numeric value to $p$ when the game ends in terminal state $s$.
\\
Also, we define two players: 1) \deff{MAX}, the agent with goal to maximise value, and 2) \deff{MIN}, the opponent with goal to minimise (agent's) value.

\subsection{Search Strategies}
\deff{MiniMax Optimal Strategy} - MAX chooses the move that maximise the minimum payoff while MIN chooses to minimise the maximum payoff.
\\
Implementation wise, it checks from leaves to root - for every node of MAX / MIN, takes the maximum / minimum utility value of its children nodes. DFS in nature.
\\
\underline{Analysis}: It is \textbf{Complete} if game tree finite; \textbf{Optimal}, assuming both players play optimally; time $O(b^m)$ and space $O(bm)$.

\smallskip

\deff{Nash Equilibrium} - a condition where whether one player knows the strategy the other player has does not affect the game outcome.


\subsection{Pruning Techniques}
\deff{Alpha-Beta Pruning} defines $\alpha(n)$ and $\beta(n)$ as highest and lowest observed value found on path from $n$ respectively. ($\alpha_0 = -\infty$ and $\beta_0 = \infty$ Initially.) The pruning follows:
\begin{itemize}
    \item Given a MIN node $n$, stop searching below $n$ if for some ancestor $i$ of $n$, $\alpha(i) \ge \beta(n)$
    \item Given a MAX node $n$, stop searching below $n$ if for some ancestor $i$ of $n$, $\beta(i) \le \alpha(n)$
\end{itemize}
\underline{Analysis}: Pruning might not improve efficiency but it does not affect correctness. \underline{Limitations}: 1) Effectiveness depends on move ordering and 2) Does not reduce maximum depth of tree. \underline{Note}: Also pruned when taking '$=$'!

\smallskip

\deff{Cutting-Off Search} with \deff{Heuristic MiniMax} prunes by replacing DFS with LDS and using a heuristic to choose (non-terminal) nodes to explore. For modelling, it 1) replaces \codein{IS-TERMINAL(s)} with \codein{CUTOFF-TEST(s, d)} that checks if current depth has reached the depth limit, and 2) replaces \codein{UTILITY(s,p)} with \codein{EVAL(s,p)}.




\section{Knowledge-Based Agents}
\deff{Knowledge-Based Agent} - agent making inferences on existing info to infer new ones. It consists of an \deff{Inference Engine} (domain-independent algorithms) and a \deff{Knowledge Base} (domain-specific content), which is a set of sentences (logical formulae).

%KB Agent & Algo.

\subsection{Logic}
A value assignment $\nu$ \deff{Models} a sentence $\alpha$ iff. $alpha$ is true under $\nu$. A \deff{Model} can also mean a value assignment.
\\
Define $M(\alpha)$ as the set of all possible models for $\alpha$.

\smallskip

For two sentences $\alpha$ and $\beta$, $\alpha$ \deff{Entails} $\beta$ iff.
\[
\alpha \models \beta \equiv M(\alpha) \subseteq M(\beta)
\]
\underline{Intuitively}, entailment is a more abstract form of "implication".

\subsection{Inference Algorithm}
Inference algorithms assume a query $\alpha$ is given and check if KB entails it. $\alpha$ usually comes from \codein{action = AS(...)}.

\subsubsection{Properties}
Let $KB \vdash_{\mathcal{A}} \alpha$ means "Sentence $\alpha$ is derived from Knowledge Base, $KB$, using inference algorithm $\mathcal{A}$".
\\
\deff{Soundness} - $KB \vdash_{\mathcal{A}} \alpha \implies KB \models \alpha$, i.e. $\mathcal{A}$ will not infer incorrect sentences.
\\
\deff{Completeness} - $KB \models \alpha \implies KB \vdash_{\mathcal{A}} \alpha$, i.e. $\mathcal{A}$ will arrive at all possible conclusions.

\subsubsection{Truth Table Enumeration Algorithm}
An algorithm that lists all $2^n$ truth assignments to verify KB entails $\alpha$, by using DFS.
\\
\underline{Analysis}: \textbf{Sound} and \textbf{Complete}. Time and space $O(2^n)$ and $O(n)$.



\section{Theorem Proving Methods}
\subsection{Logical Agent}
A sentence $\alpha$ is \deff{Valid} if it is true for \textbf{all} possible truth value assignments. It is linked to entailment by \deff{Deduction Theorem}:
\[
\big( KB \models \alpha \big) \equiv \big((KB \implies \alpha) \text{ is valid }\big)
\]

A sentence $\alpha$ is \deff{Satisfiable} if it is true for \textbf{some} truth value assignments. It is related to entailment:
\[
\big( KB \models \alpha \big) \equiv \big( (KB \land \neg \alpha) \text{ is unsatisfiable } \big)
\]

A valid sentence is a tautology. Proof by contradiction uses the idea that something is true $\equiv$ its negation is unsatisfiable.

\subsection{Resolution}
\deff{Conjunctive Normal Form} - Conjunctions of Disjunctive. i.e. clauses joined by AND, where each clause uses only OR to join literals. KB is a CNF where each clause is a sentence in KB.

\smallskip

\deff{Resolution} - Simplifies KB to prove entailment of query $\alpha$. $(R_1 \lor x) \land (R_2 \lor \neg x)$ is resolved to $R_1 \land R_2$.


\subsubsection{Resolution Algorithm}
\underline{Procedure}: 1) Make a clause list (KB in CNF, with \textbf{Negation of Query $\alpha$}). 2) Repeatedly resolve two clauses and add resolvent to list. 3) Stop when empty clause $\emptyset$ found, which implies $\alpha$ CAN be inferred, or when no more resolutions possible, which implies $\alpha$ CANNOT be inferred. 
\\
\textbf{Note}: {[1]} Cannot be inferred $\neq$ Is incorrect / negation can be inferred. {[2]} The negation of query may be a conjunction of 2 clauses (, e.g. if query has $\implies$). {[3]} When clauses have universe qualifier $\forall$, can still resolve by setting variable to certain value.
\\
\underline{Analysis}: \textbf{Sound} and \textbf{Complete}.




\section{Bayesian Inference}
\subsection{Probability Basics}
\deff{Independent Events} - $P(ABC) = P(A)P(B)P(C)$.
\\
\deff{Causal Chain} - $P(ABC) = P(C|B)P(B|A)P(A)$, where $C$ depends on $B$ and $B$ depends on $A$.
\\
\deff{Independent Causes} - $P(ABC) = P(C|AB)P(A)P(B)$, where $C$ is affected by both $A$ and $B$, which are independent.
\\
\deff{Conditionally Independent Effects} - where $A$ affects both $B$ and $C$ separately, i.e. $P(ABC) = P(C|A)P(B|A)P(A)$.

\subsection{Bayesian Network}
\deff{Bayesian Network} is a model that represents joint distributions with a Directed Graph, where vertices are random variables, and an edge from $X$  to $Y$ indicates $X$ \textbf{directly affects} $Y$. \underline{Analysis}: A Bayesian Network is a DAG.




\noindent\rule{8cm}{0.4pt}

\section{Standard Algorithm Pseudocode}
% uninformed search
\subsection{Generic Search}
\begin{lstlisting}
frontier = {initial state}
while frontier not empty:
    current = frontier.pop()
	// checking
	if isGoal(current):
		return path found
	// exploration
	for a in actions(current):
		frontier.push(T(current, a)) 
return failure
\end{lstlisting}

\subsection{CSP Backtracking}
\begin{lstlisting}
def BT-SEARCH(csp) -> solution
    return BT(csp, {})
def BT(csp, assign) -> solution
    if assign complete:
        return assign
    var = select-unassigned-var(csp, assign)
    for val in order-dom-val(csp, var, assign):
        if val is consistent:
            assign.add((var, val))
            inf = inference(csp, var, assign)
            if inf != NULL:
                csp.add(inf)
                result = BT(csp, assign)
                if result != NULL:
                    return result
                csp.remove(inf)
            assign.remove((var, val))
    return NULL
\end{lstlisting}

% AC3 (CSP)
\subsection{AC3}
\begin{lstlisting}
def AC3(csp): // true for consistent
    queue = csp.get_all_arcs()
    while queue not empty:
        (i, j) = queue.pop()
        if REVISE(csp, i, j):
            if len(D[i]) == 0:
                return FALSE
            for k in (i.neighbours() - j):
                queue.add(i,j)
    return TRUE
// true if domain revised/modified
def REVISE(csp, i, j): 
    revised = FALSE
    for x in D[i]:
        if no y in D[j] that (x,y) satisfy (i,j):
            D[i].remove(x)
            revised = TRUE
    return revised
\end{lstlisting}

\subsection{Alpha-Beta Pruning}
\begin{lstlisting}
def AB-SEARCH(game,state) -> action:
    player = game.TO-MOVE(state)
    value,move = 
        MAX-VALUE(game,state,MIN_INT,MAX_INT)
    return move
// MAX-VALUE, MIN-VALUE symmetric
def MAX-VALUE(game,state,a,b) -> (util,move):
    if game.IS-TERM(state): 
        return game.UTIL(state,player),NULL
    v = MIN_INT
    for ac in game.ACTIONS(state):
        v2,act2 = 
            MIN-VALUE(game,game.RES(state,ac),a,b)
        if v2 > v:
            v, move = v2, ac
            a = MAX(a, v)
        if v >= b: return v, move
    return v, move
\end{lstlisting}

\subsection{KB Agent Function}
\begin{lstlisting}
global: KB, t = 0 // t for time
def KB-Agent(percept) -> action:
    TELL(KB, to_sentence(percept, t))
    action = ASK(KB, make-action-query(t))
    TELL(KN, to_sentence(action, t))
    t = t + 1
    return action
\end{lstlisting}

\subsection{Truth Table Enumeration}
\begin{lstlisting}
def TT-ENTAILS(KB, a):
    symbol = list of prop. symbols in KB & a
    return TT-CHECK-ALL(KB, a, symbols, {})
def TT-CHECK-ALL(KB, a, symbols, model):
    if symbol is empty:
        if PL-TRUE?(KB, model):
            return PL-TRUE?(a, model)
        else:
            return TRUE
    // i.e. non-emtpy
    p, rest = symbols.first(), symbol.rest()
    return 
        (TT-CHECK-ALL(KB,a,rest,model+{P=TRUE})
        and 
        (TT-CHECK-ALL(KB,a,rest,model+{P=FALSE})
\end{lstlisting}

% Resolution algo.
\subsection{Resolution}
\begin{lstlisting}
def PL-RESOLUTION(KB, a) -> bool:
    clauses = get_CNF(KB+(!a))
    new = {}
    while TRUE:
        for (i, j) in clause:
            res = PL-RESOLVE(i, j)
            if res is empty:
                return TRUE
            new = union(new, res)
        if new subset of clauses: 
            // cannot further resolve
            return FALSE
        clauses = union(clauses, new)
\end{lstlisting}




\section{Intermediate Results / Lemmas}
\subsection{From Tutorials \& Quizzes}
\citeqn{(Quiz1 Qn9)} Testing goal upon pushing (than poppig) to frontier can save at most $(b^{d+1} - b)$ nodes in BFS.
\\
\citeqn{(Quiz2 Qn12)} For Uninformed Search problems with the goal node near the root, the branching factor finite, and all action costs equal, \textbf{BFS} is preferred.
\\
\citeqn{(Quiz2 Qn13)} For Uninformed Search problems with all nodes at a certain depth being goal nodes and all action costs equal, \textbf{DFS} is preferred. \underline{Note}: Distinguish between pushing order and popping order in DFS.
\\
\citeqn{(Quiz6 Qn8)} In AC3, each arc is inserted at most $d$ times for domain size $d$.
\\
\citeqn{(Tut10 Qn2)} If $T$ affects on $D$ then it is true that $D$ also depends on $T$ as our knowledge of $T$ affects info. about $D$.


\subsection{From Past Questions}
\citeqn{(AY19/20Sem1 Midterm Qn1)} A* Graph Search with consistent heuristic is guaranteed to visit no more nodes than UCS.
\\
\citeqn{(AY19/20Sem2 Midterm Qn1)} In a A* Graph Search problem, leaving a consistent heuristic $h(n)$ unchanged:
\begin{itemize}
    \item After adding edges to the transition graph, $h$ might not be still consistent.
    \item After removing edges from the transition graph, $h$ is still consistent.
\end{itemize}





\section{Summary}

\subsection{Uninformed Search Algorithms}
For Tree-Search implementation:
\begin{center}
\begin{tabular}{||c || c | c | c | c | c||} 
 \hline 
 Criterion  & BFS           & UCS           & DFS           & DLS       & IDS           \\
 \hline \hline
 Complete?  & Yes$^{[1]}$   & Yes$^{[1][2]}$& No            & No        & Yes$^{[1]}$   \\
 \hline
 Optimal?   & Yes$^{[1]}$   & Yes           & No            & No        & Yes$^{[3]}$   \\
 \hline
 Time       & $O(b^d)$  & $O(b^{1+[C^*/\epsilon]})$ & $O(b^m)$  & $O(b^l)$  & $O(b^d)$  \\
 \hline
 Space      & $O(b^d)$  & $O(b^{1+[C^*/\epsilon]})$ & $O(bm)$   & $O(bl)$   & $O(bd)$   \\
 \hline
\end{tabular}
\end{center}

For Graph-Search implementation, \textbf{all have time and space complexity $O(V+E)$}.
\begin{center}
\begin{tabular}{||c || c | c | c | c | c||} 
 \hline 
 Criterion  & BFS           & UCS           & DFS           & DLS       & IDS           \\
 \hline \hline
 Complete?  & Yes$^{[1]}$   & Yes$^{[1][2]}$& Yes$^{[1]}$   & No        & Yes$^{[1]}$  \\
 \hline
 Optimal?   & Yes$^{[1]}$   & Yes           & No            & No        & Yes$^{[3]}$   \\
 \hline
\end{tabular}
\end{center}
$[1]$ If $b$ finite \textbf{AND} (state space finite \textbf{OR} has a solution) \\
$[2]$ If the $\epsilon$ assumption is satisfied for all costs \\
$[3]$ If all costs are identical


\end{multicols}
\end{document}
